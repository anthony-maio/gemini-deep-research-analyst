{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gemini Deep Research Agent - Interactive Notebook\n",
        "\n",
        "This notebook demonstrates Google's Deep Research Agent, which autonomously plans, executes, and synthesizes multi-step research tasks.\n",
        "\n",
        "**Preview Notice:** The Deep Research Agent is free during preview. Google Search tool calls are free until January 5th, 2026.\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, install the required package and set your API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install the Google GenAI SDK\n",
        "!pip install -q google-genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from google import genai\n",
        "\n",
        "# Set your API key (get one at https://aistudio.google.com/apikey)\n",
        "# Option 1: Set directly (not recommended for sharing)\n",
        "# os.environ['GOOGLE_API_KEY'] = 'your_key_here'\n",
        "\n",
        "# Option 2: Use Colab secrets or environment variable\n",
        "# The client will automatically use GOOGLE_API_KEY from environment\n",
        "\n",
        "client = genai.Client()\n",
        "print(\"Client initialized successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Agent configuration\n",
        "DEEP_RESEARCH_AGENT = \"deep-research-pro-preview-12-2025\"\n",
        "SUMMARY_MODEL = \"gemini-2.5-flash\"\n",
        "FOLLOWUP_MODEL = \"gemini-3-pro-preview\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 1: Basic Research with Streaming\n",
        "\n",
        "The Deep Research Agent works asynchronously. We use `background=True` and `stream=True` to watch its progress in real-time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_research(query: str, file_store: str = None) -> dict:\n",
        "    \"\"\"\n",
        "    Run a deep research task with streaming output.\n",
        "    \n",
        "    Args:\n",
        "        query: The research question or task\n",
        "        file_store: Optional file store name for private document search\n",
        "    \n",
        "    Returns:\n",
        "        dict with 'text' (final report), 'thoughts' (reasoning), and 'interaction_id'\n",
        "    \"\"\"\n",
        "    # Configure tools\n",
        "    tools = None\n",
        "    if file_store:\n",
        "        tools = [{\"type\": \"file_search\", \"file_search_store_names\": [file_store]}]\n",
        "    \n",
        "    # Start the research stream\n",
        "    stream = client.interactions.create(\n",
        "        input=query,\n",
        "        agent=DEEP_RESEARCH_AGENT,\n",
        "        background=True,\n",
        "        stream=True,\n",
        "        tools=tools,\n",
        "        agent_config={\"type\": \"deep-research\", \"thinking_summaries\": \"auto\"}\n",
        "    )\n",
        "    \n",
        "    full_text = \"\"\n",
        "    thoughts = []\n",
        "    interaction_id = None\n",
        "    \n",
        "    print(\"üî¨ Research in progress...\\n\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    for chunk in stream:\n",
        "        # Capture interaction ID for follow-ups\n",
        "        if chunk.event_type == \"interaction.start\":\n",
        "            interaction_id = chunk.interaction.id\n",
        "            print(f\"üìã Interaction ID: {interaction_id}\\n\")\n",
        "        \n",
        "        # Handle content\n",
        "        if chunk.event_type == \"content.delta\":\n",
        "            if chunk.delta.type == \"text\":\n",
        "                full_text += chunk.delta.text\n",
        "                print(chunk.delta.text, end=\"\", flush=True)\n",
        "            elif chunk.delta.type == \"thought_summary\":\n",
        "                thought = chunk.delta.content.text\n",
        "                thoughts.append(thought)\n",
        "                print(f\"\\nüí≠ {thought}\\n\", flush=True)\n",
        "        \n",
        "        # Completion\n",
        "        if chunk.event_type == \"interaction.complete\":\n",
        "            print(\"\\n\" + \"=\" * 50)\n",
        "            print(\"‚úÖ Research complete!\")\n",
        "    \n",
        "    return {\n",
        "        \"text\": full_text,\n",
        "        \"thoughts\": thoughts,\n",
        "        \"interaction_id\": interaction_id\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run your research query\n",
        "# This will take several minutes - the agent is doing real research!\n",
        "\n",
        "result = run_research(\"Research the history and evolution of Google TPUs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 2: View the Agent's Reasoning\n",
        "\n",
        "The `thoughts` captured during streaming show the agent's internal reasoning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üß† Agent's Reasoning Process:\\n\")\n",
        "for i, thought in enumerate(result['thoughts'], 1):\n",
        "    print(f\"{i}. {thought}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 3: Follow-up Questions\n",
        "\n",
        "You can ask follow-up questions about completed research without re-running the entire process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ask_followup(interaction_id: str, question: str) -> str:\n",
        "    \"\"\"\n",
        "    Ask a follow-up question about a completed research session.\n",
        "    \n",
        "    Args:\n",
        "        interaction_id: The ID from the original research\n",
        "        question: Your follow-up question\n",
        "    \n",
        "    Returns:\n",
        "        The agent's response\n",
        "    \"\"\"\n",
        "    response = client.interactions.create(\n",
        "        input=question,\n",
        "        model=FOLLOWUP_MODEL,\n",
        "        previous_interaction_id=interaction_id\n",
        "    )\n",
        "    return response.outputs[-1].text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ask a follow-up (only works if you have an interaction_id from above)\n",
        "if result.get('interaction_id'):\n",
        "    followup = ask_followup(\n",
        "        result['interaction_id'],\n",
        "        \"What are the key differences between TPU v4 and v5?\"\n",
        "    )\n",
        "    print(followup)\n",
        "else:\n",
        "    print(\"No interaction ID available. Run the research cell first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 4: Structured Output with Formatting Instructions\n",
        "\n",
        "You can steer the agent's output format by providing specific instructions in your prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "structured_query = \"\"\"\n",
        "Research the competitive landscape of EV batteries.\n",
        "\n",
        "Format the output as a technical report with the following structure:\n",
        "1. Executive Summary (3-4 sentences)\n",
        "2. Key Players (include a comparison table with company, chemistry type, and capacity)\n",
        "3. Recent Developments (2024-2025)\n",
        "4. Supply Chain Considerations\n",
        "5. Outlook\n",
        "\"\"\"\n",
        "\n",
        "# Uncomment to run (takes several minutes)\n",
        "# structured_result = run_research(structured_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 5: Research with File Search (Your Own Documents)\n",
        "\n",
        "If you have a File Store set up in Google AI Studio, you can include your private documents in the research."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, create a File Store in Google AI Studio and upload your documents\n",
        "# Then use it like this:\n",
        "\n",
        "# file_search_result = run_research(\n",
        "#     query=\"Compare our Q3 report against public market analysis\",\n",
        "#     file_store=\"fileSearchStores/my-company-docs\"\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 6: Polling Instead of Streaming\n",
        "\n",
        "If you prefer not to stream, you can poll for results instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def run_research_polling(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Run research with polling instead of streaming.\n",
        "    \"\"\"\n",
        "    # Start the research\n",
        "    interaction = client.interactions.create(\n",
        "        input=query,\n",
        "        agent=DEEP_RESEARCH_AGENT,\n",
        "        background=True\n",
        "    )\n",
        "    \n",
        "    print(f\"Research started: {interaction.id}\")\n",
        "    \n",
        "    # Poll for completion\n",
        "    while True:\n",
        "        interaction = client.interactions.get(interaction.id)\n",
        "        \n",
        "        if interaction.status == \"completed\":\n",
        "            print(\"\\n‚úÖ Complete!\")\n",
        "            return interaction.outputs[-1].text\n",
        "        elif interaction.status == \"failed\":\n",
        "            raise Exception(f\"Research failed: {interaction.error}\")\n",
        "        \n",
        "        print(\".\", end=\"\", flush=True)\n",
        "        time.sleep(10)\n",
        "\n",
        "# Uncomment to run\n",
        "# polling_result = run_research_polling(\"What are the latest developments in quantum computing?\")\n",
        "# print(polling_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Summary with Gemini Flash\n",
        "\n",
        "Use a faster model to generate titles and summaries for cataloging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_summary(query: str, result_text: str) -> tuple:\n",
        "    \"\"\"\n",
        "    Generate a title and summary for a research result.\n",
        "    \n",
        "    Returns:\n",
        "        tuple of (title, summary)\n",
        "    \"\"\"\n",
        "    # Generate summary\n",
        "    summary_prompt = f\"Summarize this research in under 160 characters. Task: {query}\\nResult: {result_text[:2000]}\"\n",
        "    summary_response = client.models.generate_content(\n",
        "        model=SUMMARY_MODEL,\n",
        "        contents=summary_prompt\n",
        "    )\n",
        "    summary = summary_response.text.strip()\n",
        "    \n",
        "    # Generate title\n",
        "    title_prompt = f\"Give a 3-5 word title for this research: {query}\"\n",
        "    title_response = client.models.generate_content(\n",
        "        model=SUMMARY_MODEL,\n",
        "        contents=title_prompt\n",
        "    )\n",
        "    title = title_response.text.strip().replace('\"', '')\n",
        "    \n",
        "    return title, summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate summary for your research\n",
        "if result.get('text'):\n",
        "    title, summary = generate_summary(\"Research the history of Google TPUs\", result['text'])\n",
        "    print(f\"üìå Title: {title}\")\n",
        "    print(f\"üìù Summary: {summary}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Research to JSON\n",
        "\n",
        "Persist your research for future reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "import uuid\n",
        "\n",
        "def save_research(query: str, result: dict, title: str = None, summary: str = None):\n",
        "    \"\"\"\n",
        "    Save research to a JSON file.\n",
        "    \"\"\"\n",
        "    session_id = str(uuid.uuid4())\n",
        "    \n",
        "    data = {\n",
        "        \"id\": session_id,\n",
        "        \"interaction_id\": result.get('interaction_id'),\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"title\": title or \"Untitled Research\",\n",
        "        \"summary\": summary or query[:157] + \"...\",\n",
        "        \"prompt\": query,\n",
        "        \"response\": result['text'],\n",
        "        \"thoughts\": result['thoughts']\n",
        "    }\n",
        "    \n",
        "    filename = f\"research_{session_id[:8]}.json\"\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(data, f, indent=2)\n",
        "    \n",
        "    print(f\"üíæ Saved to {filename}\")\n",
        "    return filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save your research\n",
        "if result.get('text'):\n",
        "    filename = save_research(\n",
        "        \"Research the history of Google TPUs\",\n",
        "        result,\n",
        "        title=title if 'title' in dir() else None,\n",
        "        summary=summary if 'summary' in dir() else None\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Quick Reference\n",
        "\n",
        "| Feature | Code |\n",
        "|---------|------|\n",
        "| Basic research | `run_research(\"your query\")` |\n",
        "| With file search | `run_research(\"query\", file_store=\"fileSearchStores/name\")` |\n",
        "| Follow-up question | `ask_followup(interaction_id, \"question\")` |\n",
        "| Generate summary | `generate_summary(query, result_text)` |\n",
        "| Save to JSON | `save_research(query, result)` |\n",
        "\n",
        "**Models Used:**\n",
        "- Deep Research: `deep-research-pro-preview-12-2025`\n",
        "- Summaries: `gemini-2.5-flash`\n",
        "- Follow-ups: `gemini-3-pro-preview`\n",
        "\n",
        "**Links:**\n",
        "- [Deep Research Documentation](https://ai.google.dev/gemini-api/docs/deep-research)\n",
        "- [Interactions API](https://ai.google.dev/gemini-api/docs/interactions)\n",
        "- [Get API Key](https://aistudio.google.com/apikey)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
